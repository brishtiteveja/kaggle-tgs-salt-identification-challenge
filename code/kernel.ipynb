{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "a27a672eeb95d82e1cfd9dcb4a45ba56bacd540e"
   },
   "outputs": [],
   "source": [
    "# Links and codes\n",
    "#http://blog.kaggle.com/2017/12/22/carvana-image-masking-first-place-interview/\n",
    "# 1st place solution\n",
    "#https://github.com/asanakoy/kaggle_carvana_segmentation\n",
    "\n",
    "# 2018 nuclei data science bowl\n",
    "# https://www.kaggle.com/c/data-science-bowl-2018/data\n",
    "# winners:https://datasciencebowl.com/2018winners/\n",
    "\n",
    "# Mask rcnn by 3rd place winner: https://github.com/Gelu74/DSB_2018\n",
    "# Lots others: https://github.com/Gelu74/DSB_2018/blob/master/README.md\n",
    "\n",
    "# deeplab v3\n",
    "# https://github.com/sthalles/deeplab_v3\n",
    "# https://github.com/bonlime/keras-deeplab-v3-plus\n",
    "#https://github.com/handong1587/handong1587.github.io/blob/master/_posts/deep_learning/2015-10-09-segmentation.md\n",
    "\n",
    "#topcoders, 1st place solution\n",
    "#https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741\n",
    "# Torch solution: https://github.com/neptune-ml/open-solution-data-science-bowl-2018\n",
    "\n",
    "# 5th place keras solution (mask rcnn)\n",
    "#https://github.com/mirzaevinom/data_science_bowl_2018/blob/master/codes/model.py\n",
    "\n",
    "#http://blog.kaggle.com/2017/06/29/2017-data-science-bowl-predicting-lung-cancer-2nd-place-solution-write-up-daniel-hammack-and-julian-de-wit/\n",
    "\n",
    "# Cardiac MRI segmentation\n",
    "# Use the different models and see\n",
    "# https://github.com/chuckyee/cardiac-segmentation/tree/master/rvseg/models\n",
    "\n",
    "# 2017 national data science bowl\n",
    "# https://www.kaggle.com/c/data-science-bowl-2017#description\n",
    "# 2nd place solution for the 2017 national datascience bowl\n",
    "#http://juliandewit.github.io/kaggle-ndsb2017/\n",
    "# Top 2017 algo\n",
    "# https://datasciencebowl.com/2017algorithms/\n",
    "\n",
    "# Read lung segmentation blog\n",
    "#http://blog.kaggle.com/2017/05/16/data-science-bowl-2017-predicting-lung-cancer-solution-write-up-team-deep-breath/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa525d982fdb7e518de47cd8ec52f6ee868d312b"
   },
   "source": [
    "# Changelog\n",
    "- expanded salt coverage class to (0,100)\n",
    "- Added data augmentation methods\n",
    "- Changed uncov to uconv, but removed the dropout in the last layer\n",
    "- Corrected sanity check of predicted validation data (changed from ids_train to ids_valid)\n",
    "- Used correct mask (from original train_df) for threshold tuning (inserted y_valid_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ada861a85e9549dca27667692da408c5fdccbaa5"
   },
   "source": [
    "# About\n",
    "Since I am new to learning from image segmentation and kaggle in general I want to share my noteook.\n",
    "I saw it is similar to others as it uses the U-net approach. I want to share it anyway because:\n",
    "\n",
    "- As said, the field is new to me so I am open to suggestions.\n",
    "- It visualizes some of the steps, e.g. scaling, to learn if the methods do what I expect which might be useful to others (I call them sanity checks).\n",
    "- Added stratification by the amount of salt contained in the image.\n",
    "- Added augmentation by flipping the images along the y axes (thanks to the forum for clarification).\n",
    "- Added dropout to the model which seems to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/cent7/anaconda/5.1.0-py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/apps/cent7/anaconda/5.1.0-py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/apps/cent7/anaconda/5.1.0-py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/apps/cent7/anaconda/5.1.0-py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/apps/cent7/anaconda/5.1.0-py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/apps/cent7/anaconda/5.1.0-py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"
   },
   "source": [
    "# Params and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e54e151245d665e42bb95d9cf2e1a33cb9440e48"
   },
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "    #return img[:img_size_ori, :img_size_ori]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "530c358f2868a444e8233936996463a66c2cc4f3"
   },
   "source": [
    "# Loading of training/testing ids and depths\n",
    "Reading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"tgs-salt-identification-challenge/\"\n",
    "train_df = pd.read_csv(\"../input/\"+ dataset_path + \"train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../input/\" + dataset_path + \"depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24d7f3d982bfa582b222f012129acdda55282b6d"
   },
   "source": [
    "# Read images and masks\n",
    "Load the images and masks into the DataFrame and divide the pixel values by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b18c1f50cefd7504eae7e7b9605be3814c7cad6d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"images\"] = [np.array(load_img(\"../input/\" + dataset_path + \"train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86620c6a070571895f4f36ec050a25803915ed74",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"masks\"] = [np.array(load_img(\"../input/\" + dataset_path + \"train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1137f0a009f10b5f69e4dade5f689e744e9ce1d6"
   },
   "source": [
    "# Calculating the salt coverage and salt coverage classes\n",
    "Counting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\n",
    "Plotting the distribution of coverages and coverage classes, and the class against the raw coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18d2aa182a44c65a87c75f41047c653a79bc1c3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49c3022a6c67ba53ce0fe8637fcbdcd3149fdd45",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b13d1ecc7004832e8e042d034922796263054b7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cov_to_class(val):    \n",
    "    for i in range(0, 101):\n",
    "        if val * 100 <= i :\n",
    "            return i\n",
    "        \n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c4d3fea0e392bc3df3b364405c5d09c53dd2dec",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5e66ff4809ea2f9a679b7ddbda5028dc324137a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "sns.distplot(train_df.coverage, kde=False, ax=axs[0])\n",
    "sns.distplot(train_df.coverage_class, bins=100, kde=False, ax=axs[1])\n",
    "plt.suptitle(\"Salt coverage\")\n",
    "axs[0].set_xlabel(\"Coverage\")\n",
    "axs[1].set_xlabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dd39993eb2c7e77e5ce2d3388ea8ff1d581a670",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(train_df.coverage, train_df.coverage_class)\n",
    "plt.xlabel(\"Coverage\")\n",
    "plt.ylabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2391c568019151b098a002937516bb77a506f403"
   },
   "source": [
    "# Plotting the depth distributions\n",
    "Separatelty plotting the depth distributions for the training and the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ae7b7011b7de3caed58f9ca3939df15ffa319ad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(train_df.z, label=\"Train\")\n",
    "sns.distplot(test_df.z, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.title(\"Depth distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14835b3e0eafd3a1c0e3a1f18a2e7979e75d3fa3"
   },
   "source": [
    "# Show some example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a6bc85ee458f72c0917edf77895d5abc5eaf3ee",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_images = 60\n",
    "grid_width = 15\n",
    "grid_height = int(max_images / grid_width)\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "for i, idx in enumerate(train_df.index[:max_images]):\n",
    "    img = train_df.loc[idx].images\n",
    "    mask = train_df.loc[idx].masks\n",
    "    ax = axs[int(i / grid_width), i % grid_width]\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "plt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00655e32f93f96ebd90dbe94e35ee052f52217cd"
   },
   "source": [
    "# Create train/validation split stratified by salt coverage\n",
    "Using the salt coverage as a stratification criterion. Also show an image to check for correct upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d3c3157512d11e71ac74ce51a937b85bedfe1d1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    train_df.coverage.values,\n",
    "    train_df.z.values,\n",
    "    test_size=0.25, stratify=train_df.coverage_class, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2f1ab00f03e71e6d7f9b2214408b5a9779fc235",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\n",
    "tmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\n",
    "fix, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].imshow(tmp_img, cmap=\"Greys\")\n",
    "axs[0].set_title(\"Original image\")\n",
    "axs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\n",
    "axs[1].set_title(\"Scaled image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63ac58ab47921b4e4f54102e2c8b85fa318225f1"
   },
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a517622135321d17e4aaad749def999205da358c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_layer, start_neurons):\n",
    "    # 128 -> 64\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    # 64 -> 32\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    # 32 -> 16\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(0.5)(pool3)\n",
    "\n",
    "    # 16 -> 8\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(0.5)(pool4)\n",
    "\n",
    "    # Middle\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n",
    "\n",
    "    # 8 -> 16\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(0.5)(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "\n",
    "    # 16 -> 32\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])\n",
    "    uconv3 = Dropout(0.5)(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "\n",
    "    # 32 -> 64\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Dropout(0.5)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "\n",
    "    # 64 -> 128\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Dropout(0.5)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "\n",
    "    #uconv1 = Dropout(0.5)(uconv1)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "input_layer = Input((img_size_target, img_size_target, 1))\n",
    "output_layer = build_model(input_layer, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1aa78bd7c607e1f0e0235e4b2f82056c0361dac5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3399029adb039b049e3d6ca01fef30ed8653482b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7ded4adc1757c88a1bea59ea36b1a9f7941bd28",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c007157c2fd3d7dadcaeee2a6376351852d1e565"
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c997df11d223320536d954f5059cca38bc077be2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product, chain\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "74b9185450305def5e5b2f882b868f232081622d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relabel(img):\n",
    "    h, w = img.shape\n",
    "\n",
    "    relabel_dict = {}\n",
    "\n",
    "    for i, k in enumerate(np.unique(img)):\n",
    "        if k == 0:\n",
    "            relabel_dict[k] = 0\n",
    "        else:\n",
    "            relabel_dict[k] = i\n",
    "    for i, j in product(range(h), range(w)):\n",
    "        img[i, j] = relabel_dict[img[i, j]]\n",
    "    return img\n",
    "\n",
    "\n",
    "def relabel_random_colors(img, max_colours=1000):\n",
    "    keys = list(range(1, max_colours, 1))\n",
    "    np.random.shuffle(keys)\n",
    "    values = list(range(1, max_colours, 1))\n",
    "    np.random.shuffle(values)\n",
    "    funky_dict = {k: v for k, v in zip(keys, values)}\n",
    "    funky_dict[0] = 0\n",
    "\n",
    "    h, w = img.shape\n",
    "\n",
    "    for i, j in product(range(h), range(w)):\n",
    "        img[i, j] = funky_dict[img[i, j]]\n",
    "    return img\n",
    "\n",
    "def get_crop_pad_sequence(vertical, horizontal):\n",
    "    top = int(vertical / 2)\n",
    "    bottom = vertical - top\n",
    "    right = int(horizontal / 2)\n",
    "    left = horizontal - right\n",
    "    return (top, right, bottom, left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6cd009316425376846f137130028433ac37f7e04",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _perspective_transform_augment_images(self, images, random_state, parents, hooks):\n",
    "    result = images\n",
    "    if not self.keep_size:\n",
    "        result = list(result)\n",
    "\n",
    "    matrices, max_heights, max_widths = self._create_matrices(\n",
    "        [image.shape for image in images],\n",
    "        random_state\n",
    "    )\n",
    "\n",
    "    for i, (M, max_height, max_width) in enumerate(zip(matrices, max_heights, max_widths)):\n",
    "        warped = cv2.warpPerspective(images[i], M, (max_width, max_height))\n",
    "        if warped.ndim == 2 and images[i].ndim == 3:\n",
    "            warped = np.expand_dims(warped, 2)\n",
    "        if self.keep_size:\n",
    "            h, w = images[i].shape[0:2]\n",
    "            warped = ia.imresize_single_image(warped, (h, w))\n",
    "\n",
    "        result[i] = warped\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "iaa.PerspectiveTransform._augment_images = _perspective_transform_augment_images\n",
    "affine_seq = iaa.Sequential([\n",
    "    # General\n",
    "    iaa.SomeOf((1, 2),\n",
    "               [iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "                iaa.Affine(rotate=(0, 360),\n",
    "                           translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}, mode='symmetric'),\n",
    "                iaa.CropAndPad(percent=(-0.25, 0.25), pad_mode='symmetric')\n",
    "                ]),\n",
    "    # Deformations\n",
    "    iaa.Sometimes(0.3, iaa.PiecewiseAffine(scale=(0.02, 0.04))),\n",
    "    iaa.Sometimes(0.3, iaa.PerspectiveTransform(scale=(0.05, 0.10))),\n",
    "], random_order=True)\n",
    "\n",
    "def crop_seq(crop_size):\n",
    "    seq = iaa.Sequential([affine_seq,\n",
    "                          RandomCropFixedSize(px=crop_size)], random_order=False)\n",
    "    return seq\n",
    "\n",
    "class PadFixed(iaa.Augmenter):\n",
    "    PAD_FUNCTION = {'reflect': cv2.BORDER_REFLECT_101,\n",
    "                    'replicate': cv2.BORDER_REPLICATE,\n",
    "                    }\n",
    "\n",
    "    def __init__(self, pad=None, pad_method=None, name=None, deterministic=False, random_state=None):\n",
    "        super().__init__(name, deterministic, random_state)\n",
    "        self.pad = pad\n",
    "        self.pad_method = pad_method\n",
    "\n",
    "    def _augment_images(self, images, random_state, parents, hooks):\n",
    "        result = []\n",
    "        for i, image in enumerate(images):\n",
    "            image_pad = self._pad(image)\n",
    "            result.append(image_pad)\n",
    "        return result\n",
    "\n",
    "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n",
    "        result = []\n",
    "        return result\n",
    "\n",
    "    def _pad(self, img):\n",
    "        img_ = img.copy()\n",
    "\n",
    "        if self._is_expanded_grey_format(img):\n",
    "            img_ = np.squeeze(img_, axis=-1)\n",
    "\n",
    "        h_pad, w_pad = self.pad\n",
    "        img_ = cv2.copyMakeBorder(img_.copy(), h_pad, h_pad, w_pad, w_pad, PadFixed.PAD_FUNCTION[self.pad_method])\n",
    "\n",
    "        if self._is_expanded_grey_format(img):\n",
    "            img_ = np.expand_dims(img_, axis=-1)\n",
    "\n",
    "        return img_\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return []\n",
    "\n",
    "    def _is_expanded_grey_format(self, img):\n",
    "        if len(img.shape) == 3 and img.shape[2] == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "class RandomCropFixedSize(iaa.Augmenter):\n",
    "    def __init__(self, px=None, name=None, deterministic=False, random_state=None):\n",
    "        super(RandomCropFixedSize, self).__init__(name=name, deterministic=deterministic, random_state=random_state)\n",
    "        self.px = px\n",
    "        if isinstance(self.px, tuple):\n",
    "            self.px_h, self.px_w = self.px\n",
    "        elif isinstance(self.px, int):\n",
    "            self.px_h = self.px\n",
    "            self.px_w = self.px\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def _augment_images(self, images, random_state, parents, hooks):\n",
    "\n",
    "        result = []\n",
    "        seeds = random_state.randint(0, 10 ** 6, (len(images),))\n",
    "        for i, image in enumerate(images):\n",
    "            seed = seeds[i]\n",
    "            image_cr = self._random_crop(seed, image)\n",
    "            result.append(image_cr)\n",
    "        return result\n",
    "\n",
    "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n",
    "        result = []\n",
    "        return result\n",
    "\n",
    "    def _random_crop(self, seed, image):\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        if height > self.px_h:\n",
    "            crop_top = np.random.randint(height - self.px_h)\n",
    "        elif height == self.px_h:\n",
    "            crop_top = 0\n",
    "        else:\n",
    "            raise ValueError(\"To big crop height\")\n",
    "        crop_bottom = crop_top + self.px_h\n",
    "\n",
    "        np.random.seed(seed + 1)\n",
    "        if width > self.px_w:\n",
    "            crop_left = np.random.randint(width - self.px_w)\n",
    "        elif width == self.px_w:\n",
    "            crop_left = 0\n",
    "        else:\n",
    "            raise ValueError(\"To big crop width\")\n",
    "        crop_right = crop_left + self.px_w\n",
    "\n",
    "        if len(image.shape) == 2:\n",
    "            image_cropped = image[crop_top:crop_bottom, crop_left:crop_right]\n",
    "        else:\n",
    "            image_cropped = image[crop_top:crop_bottom, crop_left:crop_right, :]\n",
    "        return image_cropped\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class InferencePad(iaa.Augmenter):\n",
    "    def __init__(self, divisor=2, pad_mode='symmetric', name=None, deterministic=False, random_state=None):\n",
    "        super(InferencePad, self).__init__(name=name, deterministic=deterministic, random_state=random_state)\n",
    "        self.divisor = divisor\n",
    "        self.pad_mode = pad_mode\n",
    "\n",
    "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n",
    "        return keypoints_on_images\n",
    "\n",
    "    def _augment_images(self, images, random_state, parents, hooks):\n",
    "\n",
    "        result = []\n",
    "        for i, image in enumerate(images):\n",
    "            image_padded = self._pad_image(image)\n",
    "            result.append(image_padded)\n",
    "        return result\n",
    "\n",
    "    def _pad_image(self, image):\n",
    "        height = image.shape[0]\n",
    "        width = image.shape[1]\n",
    "\n",
    "        pad_sequence = self._get_pad_sequence(height, width)\n",
    "        augmenter = iaa.Pad(px=pad_sequence, keep_size=False, pad_mode=self.pad_mode)\n",
    "        return augmenter.augment_image(image)\n",
    "\n",
    "    def _get_pad_sequence(self, height, width):\n",
    "        pad_vertical = self._get_pad(height)\n",
    "        pad_horizontal = self._get_pad(width)\n",
    "        return get_crop_pad_sequence(pad_vertical, pad_horizontal)\n",
    "\n",
    "    def _get_pad(self, dim):\n",
    "        if dim % self.divisor == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.divisor - dim % self.divisor\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return [self.divisor, self.pad_mode]\n",
    "    \n",
    "def crop_seq(crop_size):\n",
    "    seq = iaa.Sequential([affine_seq,\n",
    "                          RandomCropFixedSize(px=crop_size)], random_order=False)\n",
    "    return seq\n",
    "\n",
    "\n",
    "def padding_seq(pad_size, pad_method):\n",
    "    seq = iaa.Sequential([PadFixed(pad=pad_size, pad_method=pad_method),\n",
    "                          ]).to_deterministic()\n",
    "    return seq\n",
    "\n",
    "\n",
    "def pad_to_fit_net(divisor, pad_mode, rest_of_augs=iaa.Noop()):\n",
    "    return iaa.Sequential(InferencePad(divisor, pad_mode), rest_of_augs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "124539de47dde2fbac67ed096d2ff8413371affa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Link: https://gist.github.com/oeway/2e3b989e0343f0884388ed7ed82eb3b0\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "import collections\n",
    "from PIL import Image\n",
    "import numbers\n",
    "\n",
    "def center_crop(x, center_crop_size):\n",
    "    assert x.ndim == 2\n",
    "    centerw, centerh = x.shape[0] // 2, x.shape[1] // 2\n",
    "    halfw, halfh = center_crop_size[0] // 2, center_crop_size[1] // 2\n",
    "    return x[centerw - halfw:centerw + halfw, centerh - halfh:centerh + halfh]\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    import torch\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    return torch.from_numpy(x).float()\n",
    "\n",
    "\n",
    "def random_num_generator(config, random_state=np.random):\n",
    "    if config[0] == 'uniform':\n",
    "        ret = random_state.uniform(config[1], config[2], 1)[0]\n",
    "    elif config[0] == 'lognormal':\n",
    "        ret = random_state.lognormal(config[1], config[2], 1)[0]\n",
    "    else:\n",
    "        print(config)\n",
    "        raise Exception('unsupported format')\n",
    "    return ret\n",
    "\n",
    "\n",
    "def poisson_downsampling(image, peak, random_state=np.random):\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        imgArr = np.array(image, dtype='float32')\n",
    "    else:\n",
    "        imgArr = image.astype('float32')\n",
    "    Q = imgArr.max(axis=(0)) / peak\n",
    "    if Q[0] == 0:\n",
    "        return imgArr\n",
    "    ima_lambda = imgArr / Q\n",
    "    noisy_img = random_state.poisson(lam=ima_lambda)\n",
    "    return noisy_img.astype('float32')\n",
    "\n",
    "\n",
    "def elastic_transform(image, alpha=1000, sigma=30, spline_order=1, mode='nearest', random_state=np.random):\n",
    "    \"\"\"Elastic deformation of image as described in [Simard2003]_.\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "       Proc. of the International Conference on Document Analysis and\n",
    "       Recognition, 2003.\n",
    "    \"\"\"\n",
    "    assert image.ndim == 2\n",
    "    shape = image.shape[:2]\n",
    "\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1),\n",
    "                         sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1),\n",
    "                         sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
    "    indices = [np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1))]\n",
    "    result = map_coordinates(\n",
    "            image, indices, order=spline_order, mode=mode).reshape(shape)\n",
    "    return result\n",
    "\n",
    "\n",
    "class Merge(object):\n",
    "    \"\"\"Merge a group of images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, axis=-1):\n",
    "        self.axis = axis\n",
    "\n",
    "    def __call__(self, images):\n",
    "        if isinstance(images, collections.Sequence) or isinstance(images, np.ndarray):\n",
    "            assert all([isinstance(i, np.ndarray)\n",
    "                        for i in images]), 'only numpy array is supported'\n",
    "            shapes = [list(i.shape) for i in images]\n",
    "            for s in shapes:\n",
    "                s[self.axis] = None\n",
    "            assert all([s == shapes[0] for s in shapes]\n",
    "                       ), 'shapes must be the same except the merge axis'\n",
    "            return np.concatenate(images, axis=self.axis)\n",
    "        else:\n",
    "            raise Exception(\"obj is not a sequence (list, tuple, etc)\")\n",
    "\n",
    "\n",
    "class Split(object):\n",
    "    \"\"\"Split images into individual arraies\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *slices, **kwargs):\n",
    "        assert isinstance(slices, collections.Sequence)\n",
    "        slices_ = []\n",
    "        for s in slices:\n",
    "            if isinstance(s, collections.Sequence):\n",
    "                slices_.append(slice(*s))\n",
    "            else:\n",
    "                slices_.append(s)\n",
    "        assert all([isinstance(s, slice) for s in slices_]\n",
    "                   ), 'slices must be consist of slice instances'\n",
    "        self.slices = slices_\n",
    "        self.axis = kwargs.get('axis', -1)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if isinstance(image, np.ndarray):\n",
    "            ret = []\n",
    "            for s in self.slices:\n",
    "                sl = [slice(None)] * image.ndim\n",
    "                sl[self.axis] = s\n",
    "                ret.append(image[sl])\n",
    "            return ret\n",
    "        else:\n",
    "            raise Exception(\"obj is not an numpy array\")\n",
    "\n",
    "\n",
    "class ElasticTransform(object):\n",
    "    \"\"\"Apply elastic transformation on a numpy.ndarray (H x W)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1000, sigma=30):\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if isinstance(self.alpha, collections.Sequence):\n",
    "            alpha = random_num_generator(self.alpha)\n",
    "        else:\n",
    "            alpha = self.alpha\n",
    "        if isinstance(self.sigma, collections.Sequence):\n",
    "            sigma = random_num_generator(self.sigma)\n",
    "        else:\n",
    "            sigma = self.sigma\n",
    "        return elastic_transform(image, alpha=alpha, sigma=sigma)\n",
    "\n",
    "\n",
    "class PoissonSubsampling(object):\n",
    "    \"\"\"Poisson subsampling on a numpy.ndarray (H x W x C)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, peak, random_state=np.random):\n",
    "        self.peak = peak\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if isinstance(self.peak, collections.Sequence):\n",
    "            peak = random_num_generator(\n",
    "                self.peak, random_state=self.random_state)\n",
    "        else:\n",
    "            peak = self.peak\n",
    "        return poisson_downsampling(image, peak, random_state=self.random_state)\n",
    "\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    \"\"\"Add gaussian noise to a numpy.ndarray (H x W)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, sigma, random_state=np.random):\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if isinstance(self.sigma, collections.Sequence):\n",
    "            sigma = random_num_generator(\n",
    "                self.sigma, random_state=self.random_state)\n",
    "        else:\n",
    "            sigma = self.sigma\n",
    "        if isinstance(self.mean, collections.Sequence, random_state=self.random_state):\n",
    "            mean = random_num_generator(self.mean)\n",
    "        else:\n",
    "            mean = self.mean\n",
    "        row, col = image.shape\n",
    "        gauss = self.random_state.normal(mean, sigma, (row, col))\n",
    "        gauss = gauss.reshape(row, col)\n",
    "        image += gauss\n",
    "        return image\n",
    "\n",
    "\n",
    "class AddSpeckleNoise(object):\n",
    "    \"\"\"Add speckle noise to a numpy.ndarray (H x W)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, sigma, random_state=np.random):\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if isinstance(self.sigma, collections.Sequence):\n",
    "            sigma = random_num_generator(\n",
    "                self.sigma, random_state=self.random_state)\n",
    "        else:\n",
    "            sigma = self.sigma\n",
    "        if isinstance(self.mean, collections.Sequence):\n",
    "            mean = random_num_generator(\n",
    "                self.mean, random_state=self.random_state)\n",
    "        else:\n",
    "            mean = self.mean\n",
    "        row, col = image.shape\n",
    "        gauss = self.random_state.normal(mean, sigma, (row, col))\n",
    "        gauss = gauss.reshape(row, col)\n",
    "        image += image * gauss\n",
    "        return image\n",
    "\n",
    "\n",
    "class GaussianBlurring(object):\n",
    "    \"\"\"Apply gaussian blur to a numpy.ndarray (H x W x C)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma, random_state=np.random):\n",
    "        self.sigma = sigma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if isinstance(self.sigma, collections.Sequence):\n",
    "            sigma = random_num_generator(\n",
    "                self.sigma, random_state=self.random_state)\n",
    "        else:\n",
    "            sigma = self.sigma\n",
    "        image = gaussian_filter(image, sigma=(sigma, sigma, 0))\n",
    "        return image\n",
    "\n",
    "\n",
    "class AddGaussianPoissonNoise(object):\n",
    "    \"\"\"Add poisson noise with gaussian blurred image to a numpy.ndarray (H x W x C)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma, peak, random_state=np.random):\n",
    "        self.sigma = sigma\n",
    "        self.peak = peak\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if isinstance(self.sigma, collections.Sequence):\n",
    "            sigma = random_num_generator(\n",
    "                self.sigma, random_state=self.random_state)\n",
    "        else:\n",
    "            sigma = self.sigma\n",
    "        if isinstance(self.peak, collections.Sequence):\n",
    "            peak = random_num_generator(\n",
    "                self.peak, random_state=self.random_state)\n",
    "        else:\n",
    "            peak = self.peak\n",
    "        bg = gaussian_filter(image, sigma=(sigma, sigma))\n",
    "        bg = poisson_downsampling(\n",
    "            bg, peak=peak, random_state=self.random_state)\n",
    "        return image + bg\n",
    "\n",
    "class RandomCropNumpy(object):\n",
    "    \"\"\"Crops the given numpy array at a random location to have a region of\n",
    "    the given size. size can be a tuple (target_height, target_width)\n",
    "    or an integer, in which case the target will be of a square shape (size, size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, random_state=np.random):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.shape[:2]\n",
    "        th, tw = self.size\n",
    "        if w == tw and h == th:\n",
    "            return img\n",
    "\n",
    "        x1 = self.random_state.randint(0, w - tw)\n",
    "        y1 = self.random_state.randint(0, h - th)\n",
    "        return img[x1:x1 + tw, y1: y1 + th]\n",
    "\n",
    "\n",
    "class CenterCropNumpy(object):\n",
    "    \"\"\"Crops the given numpy array at the center to have a region of\n",
    "    the given size. size can be a tuple (target_height, target_width)\n",
    "    or an integer, in which case the target will be of a square shape (size, size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.shape[:2]\n",
    "        th, tw = self.size\n",
    "        x1 = int(round((w - tw) / 2.))\n",
    "        y1 = int(round((h - th) / 2.))\n",
    "        return img[x1:x1 + tw, y1: y1 + th]\n",
    "\n",
    "\n",
    "class RandomRotate(object):\n",
    "    \"\"\"Rotate a PIL.Image or numpy.ndarray (H x W x C) randomly\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, angle_range=(0.0, 360.0), axes=(0, 1), mode='reflect', random_state=np.random):\n",
    "        assert isinstance(angle_range, tuple)\n",
    "        self.angle_range = angle_range\n",
    "        self.random_state = random_state\n",
    "        self.axes = axes\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, image):\n",
    "        angle = self.random_state.uniform(\n",
    "            self.angle_range[0], self.angle_range[1])\n",
    "        if isinstance(image, np.ndarray):\n",
    "            mi, ma = image.min(), image.max()\n",
    "            image = scipy.ndimage.interpolation.rotate(\n",
    "                image, angle, reshape=False, axes=self.axes, mode=self.mode)\n",
    "            return np.clip(image, mi, ma)\n",
    "        elif isinstance(image, Image.Image):\n",
    "            return image.rotate(angle)\n",
    "        else:\n",
    "            raise Exception('unsupported type')\n",
    "\n",
    "\n",
    "class BilinearResize(object):\n",
    "    \"\"\"Resize a PIL.Image or numpy.ndarray (H x W x C)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, zoom):\n",
    "        self.zoom = [zoom, zoom, 1]\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if isinstance(image, np.ndarray):\n",
    "            return scipy.ndimage.interpolation.zoom(image, self.zoom)\n",
    "        elif isinstance(image, Image.Image):\n",
    "            return image.resize(self.size, Image.BILINEAR)\n",
    "        else:\n",
    "            raise Exception('unsupported type')\n",
    "\n",
    "\n",
    "class EnhancedCompose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (List[Transform]): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            if isinstance(t, collections.Sequence):\n",
    "                assert isinstance(img, collections.Sequence) and len(img) == len(\n",
    "                    t), \"size of image group and transform group does not fit\"\n",
    "                tmp_ = []\n",
    "                for i, im_ in enumerate(img):\n",
    "                    if callable(t[i]):\n",
    "                        tmp_.append(t[i](im_))\n",
    "                    else:\n",
    "                        tmp_.append(im_)\n",
    "                img = tmp_\n",
    "            elif callable(t):\n",
    "                img = t(img)\n",
    "            elif t is None:\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception('unexpected type')\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d439c387da2539eb3c150a2cecfd6e242fa3b51b",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Lambda\n",
    "\n",
    "input_channel = 1\n",
    "target_channel = 1\n",
    "\n",
    "# define a transform pipeline\n",
    "transform = EnhancedCompose([\n",
    "       # Merge() #,\n",
    "        RandomCropNumpy(size=(int(img_size_target* 3/4), int(img_size_target* 3/4))),\n",
    "        RandomRotate(),\n",
    "       # Split([0, input_channel], [input_channel, input_channel+target_channel]),\n",
    "        #[CenterCropNumpy(size=(256, 256)), CenterCropNumpy(size=(256, 256))],\n",
    "        #[NormalizeNumpy(), MaxScaleNumpy(0, 1.0)],\n",
    "        # for non-pytorch usage, remove to_tensor conversion\n",
    "        #[Lambda(to_tensor), Lambda(to_tensor)]\n",
    "])\n",
    "\n",
    "import random\n",
    "# read input data for test\n",
    "ix = random.randint(0, x_train.shape[0])\n",
    "img = x_train[ix]\n",
    "print(img.shape)\n",
    "img = img.squeeze()\n",
    "img2 = x_train[ix+1].squeeze()\n",
    "imgs = []\n",
    "imgs.append(img)\n",
    "imgs.append(img2)\n",
    "\n",
    "y_img = y_train[ix].squeeze()\n",
    "\n",
    "x_en = transform(img)\n",
    "y_en = transform(y_img)\n",
    "\n",
    "c_w = int(img.shape[0]/4)\n",
    "c_h = int(img.shape[1]/4)\n",
    "\n",
    "# center crop\n",
    "img_c = center_crop(img, (c_w, c_h))\n",
    "\n",
    "# poisson downsampling\n",
    "img_ds = poisson_downsampling(img, c_w)\n",
    "\n",
    "# elastic transform\n",
    "print(img.shape)\n",
    "img_et = elastic_transform(img)\n",
    "img_exp = np.expand_dims(img_et, 2)\n",
    "print(img_exp.shape)\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(15,5))\n",
    "axs[0][0].imshow(img, cmap='Greys')\n",
    "axs[0][1].imshow(img_c, cmap='Greys')\n",
    "axs[0][2].imshow(img_ds, cmap='Greys')\n",
    "axs[0][3].imshow(img_et, cmap='Greys')\n",
    "axs[1][0].imshow(x_en, cmap='Greys')\n",
    "axs[1][1].imshow(y_img, cmap='Greys')\n",
    "axs[1][2].imshow(y_en, cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d99c68f2481a3769526a09d98f03a81257a0036",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88b3f57eac3ec3719b401730dc6d8d2d89d09ccc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "     train_df.index.values,\n",
    "     np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "     np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "     train_df.coverage.values,\n",
    "     train_df.z.values,\n",
    "     test_size=0.25, stratify=train_df.coverage_class, random_state=1337)\n",
    "\n",
    " x_train_orig = x_train\n",
    " y_train_orig = y_train\n",
    "\n",
    " print(x_train.shape)\n",
    " print(y_train.shape)\n",
    "\n",
    " x_train_t = np.zeros((x_train_orig.shape))\n",
    " for i in range(x_train_orig.shape[0]):\n",
    "     x = np.copy(x_train_orig[0])\n",
    "     #xe = elastic_transform(x.squeeze())\n",
    "     xe = poisson_downsampling(x, int(img.shape[0]/4))\n",
    "     xee = xe.reshape((x.shape[0], x.shape[1], x.shape[2]))\n",
    "     x_train_t[i] = xee\n",
    "    \n",
    " x_train = np.concatenate((x_train, x_train_t))\n",
    " print(x_train.shape)\n",
    "\n",
    " y_train_t = np.zeros((y_train_orig.shape))\n",
    " for i in range(y_train_orig.shape[0]):\n",
    "     x = np.copy(y_train_orig[0])\n",
    "     xe = elastic_transform(x.squeeze())\n",
    "     xee = xe.reshape((x.shape[0], x.shape[1], x.shape[2]))\n",
    "     y_train_t[i] = xee\n",
    "\n",
    " y_train = np.concatenate((y_train, y_train_t))\n",
    " print(y_train.shape)\n",
    "    \n",
    " # random crop, random rotate\n",
    " transform = EnhancedCompose([\n",
    "        # Merge() #,\n",
    "         RandomCropNumpy(size=(int(img_size_target* 3/4), int(img_size_target* 3/4))),\n",
    "         RandomRotate()\n",
    " ])\n",
    "\n",
    " x_train_t = np.zeros((x_train_orig.shape))\n",
    " for i in range(x_train_orig.shape[0]):\n",
    "     try:\n",
    "         x = np.copy(x_train_orig[i])\n",
    "         xn = transform(x.squeeze())\n",
    "         xnn = xn.reshape((x.shape[0], x.shape[1], x.shape[2]))\n",
    "         x_train_t[i] = xnn\n",
    "     except:\n",
    "         continue\n",
    "     i + 1\n",
    "\n",
    " x_train = np.concatenate((x_train, x_train_t))\n",
    "\n",
    " print(x_train.shape)\n",
    "\n",
    " y_train_t = np.zeros((y_train_orig.shape))\n",
    " for i in range(y_train_orig.shape[0]):\n",
    "     try:\n",
    "         x = np.copy(y_train_orig[i])\n",
    "         xn = transform(x.squeeze())\n",
    "         xnn = xn.reshape((x.shape[0], x.shape[1], x.shape[2]))\n",
    "         y_train_t[i] = xnn\n",
    "     except:\n",
    "         continue\n",
    "     i + 1\n",
    "\n",
    " y_train = np.concatenate((y_train, y_train_t))\n",
    " print(y_train.shape)      \n",
    "\n",
    " # left-right flip\n",
    " x_train = np.append(x_train, [np.fliplr(x) for x in x_train_orig], axis=0)\n",
    " y_train = np.append(y_train, [np.fliplr(x) for x in y_train_orig], axis=0)\n",
    " print(x_train.shape)\n",
    " print(y_train.shape)\n",
    "\n",
    " # up-down flip\n",
    " x_train = np.append(x_train, [np.fliplr(x) for x in x_train_orig], axis=0)\n",
    " y_train = np.append(y_train, [np.fliplr(x) for x in y_train_orig], axis=0)\n",
    " print(x_train.shape)\n",
    " print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3954983a51c08accc345a73f324c6c0b5966190a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7040f72549212dd4f71c13dfbd8bf013481ea369",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 10, figsize=(15,3))\n",
    "for i in range(10):\n",
    "    axs[0][i].imshow(x_train[i].squeeze()) #, cmap=\"Greys\")\n",
    "    axs[0][i].imshow(y_train[i].squeeze()) #, cmap=\"Greens\", alpha=0.3)\n",
    "    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze())#, cmap=\"Greys\")\n",
    "    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze())#, cmap=\"Greens\", alpha=0.3)\n",
    "fig.suptitle(\"Top row: original images, bottom row: augmented images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5a6b1abaa4681cba3b608bc5f33cf260370d82a"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1773642758da7b4480e0e48c045bd01ea3684ae",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " early_stopping = EarlyStopping(patience=10, verbose=1)\n",
    " model_checkpoint = ModelCheckpoint(\"./keras-augment-fine-strat.model\", save_best_only=True, verbose=1)\n",
    " reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    " epochs = 200\n",
    " batch_size = 32\n",
    "\n",
    " history = model.fit(x_train, y_train,\n",
    "                     validation_data=[x_valid, y_valid], \n",
    "                     epochs=epochs,\n",
    "                     batch_size=batch_size,\n",
    "                     callbacks=[early_stopping, model_checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\n",
    " ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    " ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    " ax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    " ax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c824f6bca47f051500966c433ce7fb5a9528f6d7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset2_path = 'u-net-dropout-augmentation-stratification/'\n",
    "model = load_model(\"../input/\" + dataset2_path + \"keras-augment-fine-strat.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f168318eadb324daa8c020f0e3e0a24d82a464f"
   },
   "source": [
    "# Predict the validation set to do a sanity check\n",
    "Again plot some sample images including the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\n",
    "preds_valid = np.array([downsample(x) for x in preds_valid])\n",
    "y_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])\n",
    "\n",
    "#y_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b1198b6fb7369c3cfb70e68cd1b78d36aa188bc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_images = 60\n",
    "grid_width = 15\n",
    "grid_height = int(max_images / grid_width)\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "for i, idx in enumerate(ids_valid[:max_images]):\n",
    "    img = train_df.loc[idx].images\n",
    "    mask = train_df.loc[idx].masks\n",
    "    pred = preds_valid[i]\n",
    "    ax = axs[int(i / grid_width), i % grid_width]\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n",
    "    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "plt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd973023204ebf921fe1f23748856e6a6f692aa4",
    "collapsed": true
   },
   "source": [
    "# Scoring\n",
    "Score the model and do a threshold optimization by the best IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d261beec66b6867ac0d5c94684f12aa08b70d638",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85f6d9567cec0ef8976730a6834b6569b6e108a0",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 50)\n",
    "ious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "183d37ad32bc2f1f0d17a9538702c45a826ccefc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold_best_index = np.argmax(ious[9:-10]) + 9\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ced29761f2d1760245112a30a7abd4783b373dd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "423b3268c580dc1eae84f54deeeb0f691eff6028"
   },
   "source": [
    "# Another sanity check with adjusted threshold\n",
    "Again some sample images with the adjusted threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40c263765ac6d53a8c0c1361ff1e6f061eecf825",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_images = 60\n",
    "grid_width = 15\n",
    "grid_height = int(max_images / grid_width)\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "for i, idx in enumerate(ids_valid[:max_images]):\n",
    "    img = train_df.loc[idx].images\n",
    "    mask = train_df.loc[idx].masks\n",
    "    pred = preds_valid[i]\n",
    "    ax = axs[int(i / grid_width), i % grid_width]\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n",
    "    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "plt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "332a614c0ae837c115ec6563f355753ffbb8cd83"
   },
   "source": [
    "# Submission\n",
    "Load, predict and submit the test image predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72128add82c6853441671fde67e7e66601a01787",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source https://www.kaggle.com/bguberfain/unet-with-depth\n",
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ecb152b492c7126d12c5ef2c701eec8ea3d86f1",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test = np.array([upsample(np.array(load_img(\"../input/\" + dataset_path + \"test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f278d0b87320c117b4ed7c116a991782b82ba5a7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "113f816f9db8b87ca7f6845fe6e61328ab606f41",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4243166f91c4bcb4da00208f4f53dd912dbb429f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
